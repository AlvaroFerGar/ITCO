<!DOCTYPE html>
<html lang="es">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Is The Circus Open?</title>
	<style>
		body {
			margin: 0;
			display: flex;
			flex-direction: column;
			justify-content: center;
			align-items: center;
			height: 100vh;
			background-color: #000;
			color: white;
			font-family: Arial, sans-serif;
		}

		#video {
			max-width: 100%;
			height: auto;
			transform: scaleX(-1);
			/* Flip the image for a mirror effect */
			display: none;
			/* Hide the video initially */
		}

		#canvas {
			position: absolute;
			top: 0;
			left: 0;
			max-width: 100%;
			height: auto;
			transform: scaleX(-1);
			/* Flip the canvas to match the video */
		}

		#open-camera {
			padding: 10px 20px;
			font-size: 16px;
			background-color: #007bff;
			color: white;
			border: none;
			border-radius: 5px;
			cursor: pointer;
		}

		#open-camera:disabled {
			background-color: #ccc;
			cursor: not-allowed;
		}

		#loading-text {
			font-size: 20px;
			margin-top: 20px;
		}

		#yes-text {
			display: none;
			/* Hide initially */
			font-size: 48px;
			color: #00FF00;
			position: absolute;
			top: 20px;
			left: 50%;
			transform: translateX(-50%);
			z-index: 10;
		}
	</style>
</head>

<body>
	<h1 id="instruction-text">Is the circus open?</h1>
	<button id="open-camera" disabled>Find out</button>
	<div id="loading-text">Loading circus...</div>
	<h1 id="yes-text">Yes, it is!</h1>
	<video id="video" autoplay playsinline></video>
	<canvas id="canvas"></canvas>

	<!-- face-api.js -->
	<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

	<script>
		let stream = null; // Will store the camera stream
		
		const video = document.getElementById('video');
		const canvas = document.getElementById('canvas');
		const openCameraButton = document.getElementById('open-camera');
		const instructionText = document.getElementById('instruction-text');
		const loadingText = document.getElementById('loading-text');
		const yesText = document.getElementById('yes-text');

		const currentNose = {
			position: {
				x: 0, // Nose x-coordinate in this frame
				y: 0  // Nose y-coordinate in this frame
			},
			radius: {
				x: 0, // Horizontal radius of the ellipse
				y: 0  // Vertical radius of the ellipse
			},
			shadow_offset: 0

		};

		const allowed_frame_gap = 5;
		let frame_gap = 0;

		// Request permission to access the camera when the page loads
		async function requestCameraPermission() {
			// Call the function to load the models
			loadModels().then(() => {
				console.log("Models ready to use");
			}).catch((error) => {
				console.error("Error loading models:", error);
			});

			try {
				// Check if the browser supports getUserMedia
				if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
					throw new Error('Your browser does not support camera access.');
				}

				// Request permission to access the camera
				stream = await navigator.mediaDevices.getUserMedia({
					video: {
						facingMode: 'user' // Use the selfie camera
					},
					audio: false
				});

				// Enable the button once permission is granted
				openCameraButton.disabled = false;
				openCameraButton.textContent = 'Find out!';
			} catch (error) {
				console.error('Error requesting camera permission:', error);
				alert('Could not access the camera. Make sure to allow access.');
			}
		}

		// Load face-api.js models
		async function loadModels() {
			console.log("Loading models...");
			try {
				loadingText.textContent = 'Loading circus...'; // Show loading text
				await faceapi.nets.tinyFaceDetector.load('models'); // Load the face detection model
				await faceapi.nets.faceLandmark68TinyNet.load('models'); // Load the landmarks model
				loadingText.style.display = 'none'; // Hide the loading text
				loadingText.textContent = 'Models loaded';
				console.log("Models loaded");
			} catch (error) {
				console.error('Error loading models:', error);
				loadingText.textContent = 'Error loading the model. Try reloading the page.';
			}
		}


		// Debug variables to activate more drawing elements.
		const drawFaceBox = false;
		const drawLandmarks = false;
		const drawNoseExtras = false;
		
		// Function to detect faces and landmarks in each frame
		async function detectFrame() {
			try {
				const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks(true);

				// Draw detections on the canvas
				const ctx = canvas.getContext('2d');
				ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas
				ctx.strokeStyle = '#00FF00';
				ctx.lineWidth = 2;

				
				detections.forEach(detection => {
					const { x, y, width, height } = detection.detection.box;	

					if(drawFaceBox)
					{
						ctx.strokeRect(x, y, width, height); // Draw a rectangle around the face
					}
					// Draw landmarks (facial reference points)
					const landmarks = detection.landmarks;
					
					if(drawLandmarks)
					{
						ctx.fillStyle = '#FFFFFF'; // Color for landmarks
						landmarks.positions.forEach(position => {
							ctx.beginPath();
							ctx.arc(position.x, position.y, 2, 0, 2 * Math.PI); // Draw a circle at each landmark
							ctx.fill();
						});
					}

					if(drawNoseExtras)
					{
						const noseIndices = [27, 28, 29, 30, 31, 32, 33, 34, 35]; // Nose indices
						ctx.fillStyle = '#FFFFFF'; // Change color for the nose
						noseIndices.forEach(index => {
							const noseLandmark = landmarks.positions[index];
							if (noseLandmark) {
								ctx.beginPath();
								ctx.arc(noseLandmark.x, noseLandmark.y, 2, 0, 2 * Math.PI);
								ctx.fill();
							}
						});
					}


					// Highlight the center point of the nose (index 30)
					const noseRightInImg = landmarks.positions[31];
					const noseLeftInImg = landmarks.positions[35];
					const noseCenter = landmarks.positions[30];


					if (noseCenter) {
						frame_gap = 0;
						currentNose.position.x = noseCenter.x;
						currentNose.position.y = noseCenter.y;
						currentNose.radius.x = Math.abs(noseRightInImg.x - noseLeftInImg.x)
						currentNose.radius.y = 20 * height / 140;
						currentNose.shadow_offset = +50 + (100 * (noseCenter.x - noseRightInImg.x)) / (noseRightInImg.x - noseLeftInImg.x)
						//console.log(currentNose.shadow_offset);
					}
				});

				if (detections.length == 0) {
					frame_gap++;
				}

				if (frame_gap < allowed_frame_gap) {

					//Add shadow
					ctx.beginPath();
					ctx.fillStyle = 'rgb(0, 0, 0,0.25)';
					ctx.ellipse(currentNose.position.x + currentNose.shadow_offset, currentNose.position.y,
						currentNose.radius.x, currentNose.radius.y, 0, 0, 2 * Math.PI);
					ctx.fill();


					//Create gradient
					let gradient = ctx.createRadialGradient(
					currentNose.position.x, currentNose.position.y, 0, // Coordinates and radius of the inner circle
					currentNose.position.x, currentNose.position.y, currentNose.radius.x // Coordinates and radius of the outer circle
					);
					gradient.addColorStop(0, '#FF0000'); 
					gradient.addColorStop(0.5, '#FF0000')
					gradient.addColorStop(1, '#9e3747'); 

					//Add nose
					ctx.beginPath();
					ctx.fillStyle = gradient;
					ctx.ellipse(currentNose.position.x, currentNose.position.y,
						currentNose.radius.x, currentNose.radius.y, 0, 0, 2 * Math.PI);
					ctx.fill();

					yesText.style.display = 'block'
				}
				else {
					yesText.style.display = 'none';
				}


			} catch (error) {
				console.error("Error detecting faces:", error);
			}
			// Repeat detection in the next frame
			requestAnimationFrame(detectFrame);
		}

		// Function to start the camera when the button is clicked
		async function startCamera() {
			console.log("start camera")

			if (stream) {
				video.srcObject = stream;
				video.style.display = 'block'; // Show the video
				openCameraButton.style.display = 'none'; // Hide the button
				instructionText.style.display = 'none'; // Hide the text

				// Wait for the video to be ready
				video.onloadedmetadata = async () => {

					// Get the video's position on the screen
					const videoRect = video.getBoundingClientRect();

					// Adjust the canvas position to match the video
					canvas.style.top = `${videoRect.top}px`;
					canvas.style.left = `${videoRect.left}px`
					canvas.width = video.videoWidth;
					canvas.height = video.videoHeight;

					// Start detection in each frame
					detectFrame();
				};
			}
		}

		// Assign events
		openCameraButton.addEventListener('click', startCamera);

		// Request permission when the page loads
		window.onload = requestCameraPermission;
	</script>
</body>

</html>