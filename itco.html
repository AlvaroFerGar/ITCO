<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Detección de Caras y Landmarks</title>
    <style>
        body {
            margin: 0;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: #000;
            color: white;
            font-family: Arial, sans-serif;
        }
        #video {
            max-width: 100%;
            height: auto;
            transform: scaleX(-1); /* Voltear la imagen para efecto espejo */
            display: none; /* Ocultar el video inicialmente */
        }
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            max-width: 100%;
            height: auto;
            transform: scaleX(-1); /* Voltear el canvas para coincidir con el video */
        }
        #open-camera {
            padding: 10px 20px;
            font-size: 16px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        #open-camera:disabled {
            background-color: #ccc;
            cursor: not-allowed;
        }
        #loading-text {
            font-size: 20px;
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <h1 id="instruction-text">Haz clic para abrir la cámara</h1>
    <button id="open-camera" disabled>Abrir cámara</button>
    <div id="loading-text">Cargando modelo...</div>
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>

    <!-- Incluir face-api.js -->
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

    <script>
        let stream = null; // Almacenará la transmisión de la cámara
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const openCameraButton = document.getElementById('open-camera');
        const instructionText = document.getElementById('instruction-text');
        const loadingText = document.getElementById('loading-text'); // Referencia al texto de carga
        let model = null; // Almacenará el modelo de detección

        // Solicitar permiso para acceder a la cámara al cargar la página
        async function requestCameraPermission() {

			// Llamar a la función para cargar los modelos
			loadModels().then(() => {
				console.log("Modelos listos para usar");
			}).catch((error) => {
				console.error("Error al cargar los modelos:", error);
			});
            try {
                // Verificar si el navegador soporta getUserMedia
                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    throw new Error('Tu navegador no soporta acceso a la cámara.');
                }

                // Solicitar permiso para acceder a la cámara
                stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: 'user' // Usar la cámara frontal (selfie)
                    },
                    audio: false
                });

                // Habilitar el botón una vez que se obtiene el permiso
                openCameraButton.disabled = false;
                openCameraButton.textContent = 'Abrir cámara';
            } catch (error) {
                console.error('Error al solicitar permiso para la cámara:', error);
                alert('No se pudo acceder a la cámara. Asegúrate de permitir el acceso.');
            }
        }

        // Cargar los modelos de face-api.js
        async function loadModels() {
			console.log("a cargar")
            try {
                loadingText.textContent = 'Cargando modelo...'; // Mostrar texto de carga
				console.log("a punto de cargar")
				await faceapi.nets.tinyFaceDetector.loadFromUri('/models'); // Cargar el modelo de detección de caras
                await faceapi.nets.faceLandmark68TinyNet.loadFromUri('/models'); // Cargar el modelo de landmarks
                loadingText.style.display = 'none'; // Ocultar el texto de carga
				loadingText.textContent = 'Modelos cargados';
				console.log("cargados")

            } catch (error) {
                console.error('Error al cargar los modelos:', error);
                loadingText.textContent = 'Error al cargar el modelo. Intenta recargar la página.';
            }
        }

        // Función para detectar caras y landmarks en cada frame
			async function detectFrame() {
				console.log("frame sin model")
				if (true) {
					console.log("frame con model")
					// Realizar la detección de caras y landmarks en el frame actual
					try {

						const canvasRect = canvas.getBoundingClientRect();
						const pre_detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks(true);

						// Dibujar las detecciones en el canvas
						const ctx = canvas.getContext('2d');
						ctx.clearRect(0, 0, canvas.width, canvas.height); // Limpiar el canvas
						ctx.strokeStyle = '#00FF00';
						ctx.lineWidth = 2;

						const detections = faceapi.resizeResults(pre_detections, { width: canvas.width, height: canvas.height })

						detections.forEach(detection => {
							// Asegúrate de que `detection` tiene la propiedad `box`
							if (detection && detection.box) {
								const { x, y, width, height } = detection.box;
								ctx.strokeRect(x, y, width, height); // Dibujar un rectángulo alrededor de la cara
							} else {
								console.error("Detección no válida:", detection);
							}
							// Dibujar los landmarks (puntos de referencia faciales)
							if (detection && detection.landmarks) {
								const landmarks = detection.landmarks;
								ctx.fillStyle = '#FFFFFF'; // Color para los landmarks

								landmarks.positions.forEach(position => {
									ctx.beginPath();
									ctx.arc(position.x, position.y, 2, 0, 2 * Math.PI); // Dibujar un círculo en cada landmark
									ctx.fill();
								});

								// Seleccionar y resaltar el landmark de la nariz (índices 27 a 35)
								const noseIndices = [27, 28, 29, 30, 31, 32, 33, 34, 35]; // Índices de la nariz
								ctx.fillStyle = '#FFFFFF'; // Cambiar color para la nariz

								noseIndices.forEach(index => {
									const noseLandmark = landmarks.positions[index];
									if (noseLandmark) {
										ctx.beginPath();
										// Hacer el punto de la nariz más grande (radio = 4 en lugar de 2)
										ctx.arc(noseLandmark.x, noseLandmark.y, 4, 0, 2 * Math.PI);
										ctx.fill();
									}
								});

								// Resaltar el punto central de la nariz (índice 30)
								const noseCenter = landmarks.positions[30];
								if (noseCenter) {
									ctx.fillStyle = '#FF0000'; // Cambiar color para el centro de la nariz
									ctx.beginPath();
									// Hacer el punto central de la nariz aún más grande (radio = 6)
									ctx.arc(noseCenter.x, noseCenter.y, 20, 0, 2 * Math.PI);
									ctx.fill();
								}
							}
						});

					} catch (error) {
						console.error("Error al detectar caras:", error);
					}
				}

            // Repetir la detección en el siguiente frame
            requestAnimationFrame(detectFrame);
        }

        // Función para iniciar la cámara cuando se hace clic en el botón
        async function startCamera() {
			console.log("start camera")
			//await loadModels();
            if (stream) {
				console.log("vamo")

                video.srcObject = stream;
                video.style.display = 'block'; // Mostrar el video
                openCameraButton.style.display = 'none'; // Ocultar el botón
                instructionText.style.display = 'none'; // Ocultar el texto

                // Esperar a que el video esté listo
                video.onloadedmetadata = async () => {
					canvas.top
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;

                    // Cargar los modelos de face-api.js
                    //await loadModels();

                    // Iniciar la detección en cada frame
                    detectFrame();
                };
            }
        }

        // Asignar eventos
        openCameraButton.addEventListener('click', startCamera);

        // Solicitar permiso al cargar la página
        window.onload = requestCameraPermission;
    </script>
</body>
</html>